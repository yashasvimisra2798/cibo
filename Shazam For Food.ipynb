{"cells":[{"metadata":{"_uuid":"41a6777d5e67dc652f57ce9681b4c44dc44152be","id":"uNaVQGQ9tQRr"},"cell_type":"markdown","source":"# **Shazam For Food**","execution_count":null},{"metadata":{"_uuid":"dd83ccbef866ac17c5ebe8e78dab9d5119d73b0d","id":"fBIIVGFGM6vg"},"cell_type":"markdown","source":"### **Overview** \n* **Download and extract Food 101 dataset**\n* **Understand dataset structure and files** \n* **Visualize random image from each of the 101 classes**\n* **Split the image data into train and test using train.txt and test.txt**\n* **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**\n* **Fine tune Inception Pretrained model using Food 101 dataset**\n* **Visualize accuracy and loss plots**","execution_count":null},{"metadata":{"_uuid":"201bfa1a371439fbb44da6ef4e8d232bcca0b465","id":"Xa07tVPbP7Cu"},"cell_type":"markdown","source":"### **Download and extract Food 101 Dataset**","execution_count":null},{"metadata":{"trusted":true,"_uuid":"dd82702380162e9587a0eae2f644dae2764f93c8"},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.image as img\n%matplotlib inline\nimport numpy as np\nfrom collections import defaultdict\nimport collections\nfrom shutil import copy\nfrom shutil import copytree, rmtree\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70f06e9a535b5f32ad9d927fc00e767dd72f17dd","id":"JOZZbCDoP-Hy","outputId":"99f6277e-0b8e-4541-a9b4-cce1a98f5b57","trusted":true},"cell_type":"code","source":"# Check if GPU is enabled\nprint(tf.__version__)\nprint(tf.test.gpu_device_name())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c134497b2239d3fa5f377fb5aebf1887f627233f","trusted":true},"cell_type":"code","source":"%cd /kaggle/input/food-101/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"339ff95b36c15cfb16a9316d2a19cc89aaaf7160","id":"f88XvEBTQBS9","trusted":true},"cell_type":"code","source":"# Helper function to download data and extract\ndef get_data_extract():\n  if \"food-101\" in os.listdir():\n    print(\"Dataset already exists\")\n  else:\n    print(\"Downloading the data...\")\n    !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n    print(\"Dataset downloaded!\")\n    print(\"Extracting data..\")\n    !tar xzvf food-101.tar.gz\n    print(\"Extraction done!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c36f8e6e00be58a643a598469cc3fd9a8c47a912","id":"gwLp2G9ae9xC"},"cell_type":"markdown","source":"* **Commented the below cell as the Food-101 dataset is available from Kaggle Datasets and need not be downloaded..**","execution_count":null},{"metadata":{"_uuid":"05fd6dae2dde446b3bd4a17f81a202391f9328d3","id":"O7kY0v23QJGO","outputId":"edc14855-bf55-4938-a875-af7d24729ea6","trusted":false},"cell_type":"code","source":"# Download data and extract it to folder\n# Uncomment this below line if you are on Colab\n\n#get_data_extract()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f65273b9b9b58da245745c82ac6cc5dfa54eeda","id":"eQr6hmptQe6q"},"cell_type":"markdown","source":"### **Understand dataset structure and files**","execution_count":null},{"metadata":{"_uuid":"40ac5b4a09f980a6a40dc953ed0c2b35b2439552","id":"n0xi2zwVQsWq"},"cell_type":"markdown","source":"**The dataset being used is [Food 101](https://www.vision.ee.ethz.ch/datasets_extra/food-101/)**\n* **This dataset has 101000 images in total. It's a food dataset with 101 categories(multiclass)**\n* **Each type of food has 750 training samples and 250 test samples**\n* **Note found on the webpage of the dataset :  **  \n***On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.***  \n* **The entire dataset is 5GB in size**","execution_count":null},{"metadata":{"_uuid":"7ceb7f04d55f3d1510a03868713e773e7df17046","id":"7wJ_OH1DQyrd","outputId":"bda2768c-24b9-4962-b7eb-fb3b07d6fad4","trusted":true},"cell_type":"code","source":"# Check the extracted dataset folder\n!ls food-101/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d831522d781ad9711f592c3d33de4c58a02a9a36","id":"9M2OZ8O_RVhu"},"cell_type":"markdown","source":"**images** folder contains 101 folders with 1000 images  each  \nEach folder contains images of a specific food class","execution_count":null},{"metadata":{"_uuid":"9a3e091f8d5db4be5a553a2fd23970dde7c649f2","id":"yy_pAK35Rbdi","outputId":"9b374f07-961a-4878-85a2-86589b2f68cb","trusted":true},"cell_type":"code","source":"os.listdir('food-101/images')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2decce84b3d9ffa491d19ded5357805061604749","id":"Ld4DOKjzSdns"},"cell_type":"markdown","source":"**meta** folder contains the text files - train.txt and test.txt  \n**train.txt** contains the list of images that belong to training set  \n**test.txt** contains the list of images that belong to test set  \n**classes.txt** contains the list of all classes of food","execution_count":null},{"metadata":{"_uuid":"c3f4490c55a470c25cf5f92f131f21701782e645","id":"jdIDm6tkSwqY","outputId":"8a3b9226-69ff-4bba-89fc-b303d1e10d7f","trusted":true},"cell_type":"code","source":"os.listdir('food-101/meta')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d938790622795157c456e6411999c571ccf64abe","id":"55WJA5RTQtuL","outputId":"d0302ba3-7d58-43c3-bd13-93b1696a9749","trusted":true},"cell_type":"code","source":"!head food-101/meta/train.txt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f606b4a970498cf9303c2d393605494b51bc3f6","id":"a3yfov0gQocW","outputId":"8ee3f518-986e-4135-e987-c61cd9b5b53c","trusted":true},"cell_type":"code","source":"!head food-101/meta/classes.txt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4af37d46992f3b2aa7b74efcb5102751b167914e","id":"motIEZu_TVih"},"cell_type":"markdown","source":"### **Visualize random image from each of the 101 classes**","execution_count":null},{"metadata":{"_uuid":"da6910e8bb064b76c17e07f0a2e0e23ebdefbbfa","id":"Jfif27Pr5KEn","outputId":"a451ddd4-2beb-4d36-eb00-000ed2f23286","trusted":true},"cell_type":"code","source":"# Visualize the data, showing one image per class from 101 classes\nrows = 17\ncols = 6\nfig, ax = plt.subplots(rows, cols, figsize=(25,25))\nfig.suptitle(\"Showing one random image from each class\", y=1.05, fontsize=24) # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\ndata_dir = \"food-101/images/\"\nfoods_sorted = sorted(os.listdir(data_dir))\nfood_id = 0\nfor i in range(rows):\n  for j in range(cols):\n    try:\n      food_selected = foods_sorted[food_id] \n      food_id += 1\n    except:\n      break\n    if food_selected == '.DS_Store':\n        continue\n    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n    ax[i][j].imshow(img)\n    ax[i][j].set_title(food_selected, pad = 10)\n    \nplt.setp(ax, xticks=[],yticks=[])\nplt.tight_layout()\n# https://matplotlib.org/users/tight_layout_guide.html","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a8300cde1146c50e672079b44346e723d813702","id":"KIgareCETmct"},"cell_type":"markdown","source":"### **Split the image data into train and test using train.txt and test.txt**","execution_count":null},{"metadata":{"_uuid":"d6c87e52c1af18e3243f1cb72b5834941828b286","id":"xB0XMUX_5KMQ","trusted":true},"cell_type":"code","source":"# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src,dest):\n  classes_images = defaultdict(list)\n  with open(filepath, 'r') as txt:\n      paths = [read.strip() for read in txt.readlines()]\n      for p in paths:\n        food = p.split('/')\n        classes_images[food[0]].append(food[1] + '.jpg')\n\n  for food in classes_images.keys():\n    print(\"\\nCopying images into \",food)\n    if not os.path.exists(os.path.join(dest,food)):\n      os.makedirs(os.path.join(dest,food))\n    for i in classes_images[food]:\n      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n  print(\"Copying Done!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"786cdb3d60ebcdddb4c2875f9e0b4508c5210fc1","id":"LSgcYcqy5KUd","outputId":"7e65498b-bcd8-4209-87e8-bdc1a8c37b4e","trusted":true},"cell_type":"code","source":"# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n%cd /\nprint(\"Creating train data...\")\nprepare_data('/kaggle/input/food-101/food-101/meta/train.txt', '/kaggle/input/food-101/food-101/images', 'train')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba85577e0ef10c768e000ecf32dee36566d52599","id":"JI65wZgT5Kb-","outputId":"2e71e6bc-43de-4ea3-f5d6-a660c4a3dc42","trusted":true},"cell_type":"code","source":"# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\nprint(\"Creating test data...\")\nprepare_data('/kaggle/input/food-101/food-101/meta/test.txt', '/kaggle/input/food-101/food-101/images', 'test')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5a96c42b2af54aa994b86a58d2657b81786381b","id":"Xccc8PJP5K1G","outputId":"981ab583-491f-41ff-a128-d1f29c137775","trusted":true},"cell_type":"code","source":"# Check how many files are in the train folder\nprint(\"Total number of samples in train folder\")\n!find train -type d -or -type f -printf '.' | wc -c","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"492bb30a200d6cde501fbe2a6424c1c05a67c2e8","id":"Iz3fjQw25K3-","outputId":"b667062f-9acb-4be7-81ff-15347abdc750","trusted":true},"cell_type":"code","source":"# Check how many files are in the test folder\nprint(\"Total number of samples in test folder\")\n!find test -type d -or -type f -printf '.' | wc -c","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7ee945bd17a45706d665517a51f5adc1b6b9106","id":"O5rLWIHpUGWf"},"cell_type":"markdown","source":"### **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**","execution_count":null},{"metadata":{"_uuid":"73bee6bd67467a34c75d2577090393735356f10d","id":"q5N8FCksUWf6"},"cell_type":"markdown","source":"* We now have train and test data ready  \n* But to experiment and try different architectures, working on the whole data with 101 classes takes a lot of time and computation  \n* To proceed with further experiments, I am creating train_min and test_mini, limiting the dataset to 3 classes  \n* Since the original problem is multiclass classification which makes key aspects of architectural decisions different from that of binary classification, choosing 3 classes is a good start instead of 2","execution_count":null},{"metadata":{"_uuid":"27bf6dfb6b5c0a01efaad5de5ac90301663e84ca","id":"b9i8vGHYKO-g","outputId":"b810d29e-bdad-4aa1-cc66-3737c075408f","trusted":true},"cell_type":"code","source":"# List of all 101 types of foods(sorted alphabetically)\ndel foods_sorted[0] # remove .DS_Store from the list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61dfaaf6466e5677c0c732c9133561e27d048ef0","trusted":true},"cell_type":"code","source":"foods_sorted","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6101991b0ff433a76afdc2cb99c021862c83860e","id":"tYyJGTJ6J9CP","trusted":true},"cell_type":"code","source":"# Helper method to create train_mini and test_mini data samples\ndef dataset_mini(food_list, src, dest):\n  if os.path.exists(dest):\n    rmtree(dest) # removing dataset_mini(if it already exists) folders so that we will have only the classes that we want\n  os.makedirs(dest)\n  for food_item in food_list :\n    print(\"Copying images into\",food_item)\n    copytree(os.path.join(src,food_item), os.path.join(dest,food_item))\n      ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71c58df71a49e4e0e2e0bfe6359602c92c2f2306","id":"9YAscZLV5LFK","trusted":true},"cell_type":"code","source":"# picking 3 food items and generating separate data folders for the same\nfood_list = ['hamburger','pizza', 'sushi']\nsrc_train = 'train'\ndest_train = 'train_mini'\nsrc_test = 'test'\ndest_test = 'test_mini'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6d67301fb0a709613c1c7a3f00629c349065e3","id":"tvlXbJ3NoPzy","outputId":"7a4173c9-9328-421e-e65f-0a25ecf55591","trusted":true},"cell_type":"code","source":"print(\"Creating train data folder with new classes\")\ndataset_mini(food_list, src_train, dest_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"274b3f850418d859fdc0cffbbcbf8d33bdd8789b","id":"t7mWJCev5LI8","outputId":"76eab9e3-ca12-4cda-a551-e2629d0cb06e","trusted":true},"cell_type":"code","source":"print(\"Total number of samples in train folder\")\n\n!find train_mini -type d -or -type f -printf '.' | wc -c","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93fb0d0f94b56dd488ebe3e4964b9799cf5228b3","id":"s4aeURey5LLy","outputId":"8a3a62c6-852e-48ae-ab5c-9d3b86eaf082","trusted":true},"cell_type":"code","source":"print(\"Creating test data folder with new classes\")\ndataset_mini(food_list, src_test, dest_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9560f0dd98c3decf8fceed05072def71efc2462e","id":"LBLq_gYD5LOm","outputId":"7af18bcd-a0e3-437d-c26b-b83e726d48b4","trusted":true},"cell_type":"code","source":"print(\"Total number of samples in test folder\")\n!find test_mini -type d -or -type f -printf '.' | wc -c","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a9e9afbfe1fc303c23c06c27444b66988ab8e9d","id":"upx61ukJiA8B"},"cell_type":"markdown","source":"### **Fine tune Inception Pretrained model using Food 101 dataset**","execution_count":null},{"metadata":{"_uuid":"b9b7bc7c0a9ca43a9b0a2d1c77e11bd40f305ae9","id":"z5hh8fj8iIaV"},"cell_type":"markdown","source":"* Keras and other Deep Learning libraries provide pretrained models  \n* These are deep neural networks with efficient architectures(like VGG,Inception,ResNet) that are already trained on datasets like ImageNet  \n* Using these pretrained models, we can use the already learned weights and add few layers on top to finetune the model to our new data  \n* This helps in faster convergance and saves time and computation when compared to models trained from scratch","execution_count":null},{"metadata":{"_uuid":"131cda47a01c9a62a8c9563c74a2b8b3545e392a","id":"8AzJVmphi0VQ"},"cell_type":"markdown","source":"* We currently have a subset of dataset with 3 classes - hamburger, pizza and shushi  \n* Use the below code to finetune Inceptionv3 pretrained model","execution_count":null},{"metadata":{"_uuid":"8d08ece78ab731f7ac8a9b4b581e42e29093bcca","id":"JBs1U7hZkp1U","outputId":"83b079b8-2550-41db-fcac-961fb11c07fc","trusted":true},"cell_type":"code","source":"K.clear_session()\nn_classes = 3\nimg_width, img_height = 299, 299\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\nnb_train_samples = 2250 #75750\nnb_validation_samples = 750 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_3class.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples // batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples // batch_size,\n                    epochs=5,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('/kaggle/working/model_trained_3class.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('/kaggle/working/model_trained_3class.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"558180f917fba895b9792328b234421c097f1ba0"},"cell_type":"code","source":"class_map_3 = train_generator.class_indices\nclass_map_3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"732521c21ce35b5be0d571cbe9d392d93755671c","id":"KbDzLAHGpJXQ"},"cell_type":"markdown","source":"### **Visualize the accuracy and loss plots**","execution_count":null},{"metadata":{"_uuid":"27084e072edb2c961bfa6be87d80f273d32533c2","id":"SjRm_AWZpPZm","outputId":"42a0123b-d15d-4ffa-cba4-e92cd81b0324","trusted":true},"cell_type":"code","source":"def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a73c830d2fbd841c9541628061313e4fd8506a51","trusted":true},"cell_type":"code","source":"plot_accuracy(history,'FOOD101-Inceptionv3')\nplot_loss(history,'FOOD101-Inceptionv3')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9b8c3c858073a21b67290e25e6c34bdf50fedea","id":"mQnoYY2ZPRrf"},"cell_type":"markdown","source":"* **The plots show that the accuracy of the model increased with epochs and the loss has decreased**\n* **Validation accuracy has been on the higher side than training accuracy for many epochs**\n* **This could be for several reasons:**\n  * We used a pretrained model trained on ImageNet which contains data from a variety of classes\n  * Using dropout can lead to a higher validation accuracy\n\n \n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}